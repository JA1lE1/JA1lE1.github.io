---
layout:     post
title:      论文阅读--The Optimality of Naive Bayes
subtitle:   朴素贝叶斯的可行性分析
date:       2019-4-15
author:     JAYLEE
header-img: img/Thomas_Bayes.png
catalog: true
tags:
    - 论文
    - 机器学习
    - 朴素贝叶斯
---

# 论文阅读--The Optimality of Naive Bayes

​							![image title](https://img.shields.io/badge/auther-JA1LE1-orange.svg)		 ![image title](https://img.shields.io/badge/Weekly-Blog-brightgreen.svg)

## 写在前面（直接忽略:runner:）

- 这篇论文是加拿大New Brunswick大学教授Harry Zhang的一篇论文。是有关朴素贝叶斯的可行性分析的一篇文章。
- 论文来源--厦门大学人工智能课程的汇报作业

## Abstract

- Naive Bayes 属于 Generative Learning Algorithms，或者是Probabilistic Generative Model 。它在机器学习和数据挖掘方面都有很显著的效果，特别是在分类任务方面有十分优秀的表现。但是它的前提条件——条件独立性假设，几乎在实际应用中是不存在的。一个Open Question的问题是，Naive Bayes 在分类方面的优异表现背后的真正原因究竟是什么呢？
- 这篇文章引入了一种关于朴素贝叶斯在分类方面的优异表现的全新的解释。文中表示出了数据特征之间的相关性分布启到了十分重要的作用。例如每个类别的局部关联性（==均匀或者不均匀？==） 和当所有的特征结合起来后的局部关联效果，（与原来效果一致或者打破原来的相关性）。因此无论数据特征之间的相关性强度有多大，如果，特征之间的独立性分布==是均匀分布==，或者是特征之间的关联性被总体效果所打破，朴素贝叶斯仍然能够保持最理想的表现。文中提出并证明了一种朴素贝叶斯最优性的充分必要条件。并且在文章的最后研究了数据在高斯分布下的最优性原则。并提出并证明了在特征之间的相关性存在的情况下，朴素贝叶斯的充分条件。而这项探究也表明了在多个数据特征联合起作用时，存在于特征之间原本的相关性可能会消失！另外，论文还探究出了==朴素贝叶斯works well 的时候==

## Naive Bayes and Augmented Naive Bayes

- 分类任务是机器学习和数据挖掘领域的一个基本的话题。在分类任务中，学习算法的目标是构造器基于带有标签的训练数据集的分类器。这里以E代表数据特征值组（$$x_1,x_2,x_3.....x_n$$）。$$x_i$$代表的是特征$$X_i$$ 的值。C代表的是分类变量。c表示C的值。在文中提出的是二分类任务，因此只有+，-标签。

-   分类器是赋予数据与标签的函数，从概率的角度上来讲，对于数据E（$$x_1,x_2,x_3.....x_n​$$）,它被分类成c的概率由以下公式表示：
  $$
  p(c|E) = \frac{p(E|c)p(c)}{p(E)}
  $$
  

- E被分类为C = +,当且仅当
  $$
  f_b(E) = \frac{p(C = +|E)}{p(C = -|E)} \geq 1
  $$
  

- $$f_b(E)$$ 就是贝叶斯分类器

- 而朴素贝叶斯基于特征之间的独立性前提，因此
  $$
  p(E|C) = P(X_1,x_2,.....,x_n|c) = \prod_{i=1}^{n}p(x_i|c)
  $$

- 而贝叶斯分类器的表示基于如下表示：

$$
f_nb(E) = \frac{p(C = +)}{P(C =-)}\prod_{i=1}^{n}\frac{p(x_i|C = +)}{P(X_i|C = -)}
$$

- 对应的朴素贝叶斯图模型如下：

![navie-bayes 概率图](E:\work\github\JA1lE1.github.io\My_Daily_Work\笔记\2019\4月\pictures\Naive bayes\navie-bayes 概率图.png)

- 朴素贝叶斯是贝叶斯网络的最简单的形式，它的各个特征之间均相互独立，而这基本上是很少存在于实际当中的，而一种最直接的解决方法就是拓展它的结构，使用ANB（ augmented naive Bayes）替代它。它的概率图如下表示：

![an example of ANB](E:\work\github\JA1lE1.github.io\My_Daily_Work\笔记\2019\4月\pictures\Naive bayes\an example of ANB.png)

- 可以看出他的类别节点C指向所有的节点。并且特征节点时间有连接关系。依概率的角度上来讲，ANB G 可以表示成如下的联合概率分布表示：
  $$
  p_G(x_1,.....,x_n,c) = p(c)\prod{i=1}^{n}p(x_i|pa(x_i),c)
  $$

- 其中，$$pa(x_i）代表X_i 父节点的值（不包括C）$$
- 文中提出作者本人在2001年的一篇文章中指出任何的贝叶斯网络都能够用ANB来表示，因此任何的联合概率分布都能够用ANB来表示。
- 由于$$log f_b(E) 和 f_b(E) 的分类效果是一样的，因此文中中都采用log型分类器 $$

## Related Work

- 已经有研究表明（D. P. 1997）尽管分类准确率不依赖于热衷之间的相关性，例如朴素贝叶斯在强相关的特征存在的条件下依然能够表现出好的分类效果。

- D.P.将朴素贝叶斯归因于它的损失函数—<0-1损失函数>,这个损失函数 的定义如下表示：

  **Zero**-**one loss** is a common **loss function** used with classification learning. It assigns 0 to **loss** for a correct classification and 1 for an incorrect classification.

- 损失函数记录了错误的次数，它不像均方差等其他函数一样，把不正确的概率估计作为惩罚项，只要正确的类别有最大的概率就可以了。这就意味着朴素贝叶斯经常会改变类别的后验概率，但是最终的分类效果其实是不变的。当然这也意味这朴素贝叶斯用来做回归分析效果可能会很差。例如如果原来的正确分类概率是+1;90%,-1:10,而使用朴素贝叶斯可能计算出的后验概率是0.6和0.4，但是它的分类效果依然达成，但是概率估计就很差劲了。
- 但是D.P.等人的发现依然是十分浅显的，因为他们没有说明，为什么在强相关的数据集下，朴素贝叶斯的分类结果没有变差。而我们需要知道的是数据之间的相关性是如何影响到分类的结果，并且下什么条件下，相关性会影响到分类结果。当前文献（基于作者写作），没有提出朴素贝叶斯最优性的确切条件。
- 文中所提最关键点在于，朴素贝叶斯事实上受到数据间的相关性分布的影响而不是数据之间相关性，这句话怎么理解呢？例如有两个特征是相关的，但是当所有的数据联合时，这种相关性就会消失。文中还提出了在高斯分布的情况下，朴素贝叶斯最优性的充分条件，并且提出了朴素贝叶斯的works well 的条件。

## A New Explanation on the Superb Classification Performance of Naive Bayes

- 在给定数据集中，两个变量具有相关性，但是相关分布在每个类别中是均匀的。在这种情况下，数据的独立性条件显然是不存在啊的，但是朴素贝叶斯仍然可以做好很好的分类效果。并且，可以说最终影响分类效果的是所有特征相关性的结合。如果我们仅仅观察两个特征的相关性关系，可以说他们确实可能会影响到分类的效果，但是当所有的特征结合到一起，数据之间的相关性可能会因此消失并不再影响最终的分类情况。因此文章提出，是包含所有数据的相关性分布影响了朴素贝叶斯的分类效果，而不是单独的相关性导致的。
- **Definition 1**
  - 给条数据E,两个分类器$$f_1和f_2$$在0-1损失函数的条件下等价的条件是。当$$f_1(E)\geq 0 并且当且仅当f_2(E)\geq 0 ,并且此时我们用f_1(E) = f_2(E) 来表示这种情况$$ ，并且如果在整个数据空间中，所有的数据E都能够满足$$f_1(E) = f_2(E)$$ ,就用$$f_1 = f_2 $$ 来表示。

### Local Dependence Distribution

- 前文中提到ANB可以表示成所有的联合概率分布，因此我们可以选择ANB作为潜在的概率分布来表示。我们要探究的是，在什么条件下，朴素贝叶斯和ANB是等价的效果

- 我们假定ANB G 只有两个类别，并且特征之间的相关性用节点之间的有向弧来表示。对于每个节点，它的父节点对它的影响是用对用的条件概率来衡量的。我们称节点和父节点之间的相关性为**局部相关性** 那么如何去衡量每个节点的局部相关性呢？基于父节点条件下的节点的条件概率和不包含父节点的条件概率的比值将能够衡量父节点对于子节点在分类的效果影响程度，并且有了以下的定义：

- **Definition 2**

  - 对于ANB G 中的节点X，它的局部独立性在正负类上的表现可以用以下两个算式表示：
    $$
    dd^+_G(x|pa(x)) = \frac{p(x|pa(x),+)}{p(x|+)}\\
    dd^-_G(x|pa(x)) = \frac{p(x|pa(x),-)}{p(x|-)}
    $$

- $$dd^+_G$$ 反应出了节点X在+类别上的相关性强度，用以衡量X的局部相关性在分类出+的影响程度。而$$dd^-_G$$是类似的，进一步可以得出以下的结果

  - 当节点X没父节点的情况下
    $$
    dd^+_G(x|pa(x)) = dd^-_G(x|pa(x)) =1
    $$

  - 当$$dd^-_G(x|pa(x)) \geq 1$$ X 在类别+下的局部相关性支持分类出C = + 的结果，否则支持分类出C = - 的结果。类似的$$dd^-_G(x|pa(x))\geq 1$$ 就是在类别-下支持分了出类别C= -,否则支持分类出C = +.直观上来说，当从两个类别导出的局部相关性支持不同类别的分类结果。在两个类别的局部相关性将会相互抵消。而另外一种请求就是局部相关性将会联合支持分类的结果。

- 以上的讨论显示出了从两个类别导出的局部相关性的比值将最终决定了每个节点的局部相关性支持的分类结果。因此有以下的定义

- **Definition 3**

  - 对于ANB G 中的一个节点X,在节点X的局部相关性导出率，用$$ddr_G(x)$$ 表示，如下所示：

  $$
  ddr_G(X) = \frac{dd^+_G(x|pa(x))}{dd^-_G(x|pa(x))}
  $$

  - 在以上的定义中，$$ddr_G(x)$$ 衡量了X的局部相关性在分类结果上的影响。更进一步，我么得到以下的结果。
    1. 如果X没有父节点，$$ddr_G(x) = 1$$
    2. 如果$$dd^+_G(x|pa(x)) = dd^-_G(x|pa(x)) 得出 ddr_G(x) = 1$$ 这意味着x的局部相关分布在类别-和类别+是均匀的。因此无论相关性的强度多大，局部相关性都不会影响最后的分类的效果。 
    3. 如果 $$ddr_G(X) > 1$$ X在类别+的局部相关性将会比在类别-上面的局部相关性强，如果$$ddr_G <1$$ 将会得到相反的结果。

### Global Dependence Distribution

- 现在我们来探究在什么样的情况下ANB将会和naive bayes 得到一样的分类结果。下面的**定理**建立起了一个ANB 和naive bayes的关系

#### Theorem 1

给定一个 基于特征（$$x_1,x_2,......x_n$$）ANB 概率模型图G，和它对应的朴素贝叶斯概率图$$G_nb$$ ,假定$$f_b 和 f_nb 分别是 G 和 G_nb 对应的分类器$$  对于给定的特征数据E($$x_1,x_2，......x_n$$) 以下等式是恒成立的。
$$
f_b(x_1,x_2,.....,x_n) = f_nb(x_1,x_2,......x_n)\prod_{i=1}^{n}ddr_G(x_i)
$$
其中$$\prod_{i=1}^{n}ddr_G(x_i)$$ 被称为特征数据E的相关分布因素，用$$DF_G(E)$$ 来表示。

==证明过程（未）==

- 从定理1中可以看出，相关性分布因素$$DF_G(E)$$ 决定了ANB和它对应的朴素贝叶斯在分类任务上的差异。更进一步，$$DF_G(E) $$ 是所有节点的局部相关导出率的产物。因此它反映出了全局相关分布（在每个类别上局部相关性分布，以及所有局部相关性的结合效果）。例如，当$$df_G(E) = 1$$ ,G将会和$$G_nb$$ 有一样的分类效果。事实上，这里并不一定要求$$DF_G = 1$$ 为了使得ANB G 与对应的朴素贝叶斯$$G_nb$$ 有一样的效果，给出了以下的定理

#### Theorem 2

给定特征数据E($$x_1,x_2,.....x_n$$) 一个ANB G 在0-1损失函数的条件下，与对应的朴素贝叶斯$$G_nb$$ 等价。例如$$f_b(E) = f_nb(E) $$ 当且仅当$$f_b(E) \geq 1, DF_G\leq f_b(E)$$ 或者当$$f_b(E)<1,DF_G(E)\geq f_b(E)$$

- 对于**定理2** 如果特征中的相关性分布满足确切的条件，朴素贝叶斯将与潜在的ANB效果是一致的。（即使存在强大的相关性），更进一步，我们可以得出以下的结果。
  1. 当$$DF_G(E) = 1$$ ANB G 的相关性将不会影响到分类效果。也就是G的分类效果将会和$$G_nb$$ 的分类效果是一样的。以下展示出的是3种$$DF_G(E) = 1$$ 的情况
     - 特征之间没有相关性
     - 对于G图上的每个节点X，$$ddr_G(x) = 1$$ ,也就是说，每个节点的局部相关性分布在两个类别上是平均的。
     - 一些支持对E分类成C = +的局部相关因素被其他支持E分类成C= - 的局部相关因素所抵消
  2. $$f_b(E)  = f_nb(E)$$ 并不要求$$DF_G(E)  = 1$$ 定理2的准确性条件解释了为什么在数据特征之间存在强相关性的数据集下，朴素贝叶斯依然能够有很好地分类效果。
  3. 当且仅当定理2的条件非真的条件下，ANB的相关性将会导致对应的朴素贝叶斯分类结果的改变。（也就是G和$$G_nb$$的分类结果将会不同）

-  定理2提出了基于特征数据E下，朴素贝叶斯最优性的充分必要条件。对于数据空间中的每个数据E,$$f_b(E) = f_nb(E) 也就是 f_b = f_nb(上文有对应的定义)$$ 朴素贝叶斯将具有全局最优性。





## Conditions for the Optimality of Naive Bayes

- 这一小节中，我们将提出如果数据特征之间的相关性被相互抵消，朴素贝叶斯能够达到最优性。也就是即使在相关性存在的条件下，朴素贝叶斯仍然能够达到最优性表现。在这一节中，将研究特征在多元高斯分布的环境下，假定特征之间的相关性存在，朴素贝叶斯最优性的充分条件。这给我们提供了特征之间的相关性可能会相互抵消的理论依据。

- 这里限制讨论的范围在特征$$X_1 和 X_2$$ 下，并且假定类别的概率密度函数是一个多元高斯分布。也就是如下算式表示：

   
  $$
  
  $$
  